# -*- coding: utf-8 -*-
"""Crypto_Project_Script_Model_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WgMBSSIoB-MKctpoUSL9OnmIJ9w5TUOU

# **Ethereum Price Prediction using Time2Vec**

### References

### Reseach Papers
https://arxiv.org/pdf/1907.05321.pdf
https://openreview.net/attachment?id=rklklCVYvB&name=original_pdf

### Kaggle codes
https://www.kaggle.com/danofer/time2vec-water-levels

### Github codes
https://github.com/email81227/Time2Vec-TensorFlow2/tree/master/Time2Vec
https://github.com/cerlymarco/keras-hypetune
https://github.com/tensorflow/addons/issues/2508
https://github.com/cerlymarco/MEDIUM_NoteBook/blob/master/Time2Vec/Time2Vec.ipynb

### Towards Data Science articles
https://towardsdatascience.com/neural-networks-with-sine-basis-function-c5c13fd63513 (paper on sine basis function)
https://towardsdatascience.com/time2vec-for-time-series-features-encoding-a03a4f3f937e

### Implementation of the model
"""

!pip install yfinance

!pip install keras-hypetune

# Data Manipulation
import numpy as np
import pandas as pd

# Plotting graphs
import matplotlib.pyplot as plt

# Machine learning
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
from tensorflow.keras.layers import Bidirectional, Dropout, Activation, Dense, LSTM
from tensorflow.keras.models import Sequential
from kerashypetune import KerasGridSearch

# Data fetching
from pandas_datareader import data as pdr
import yfinance as yf
yf.pdr_override()



df = pdr.get_data_yahoo('ETH-USD', '2016-01-01', '2021-07-19')

df = df.dropna()

df.info()

df = df.reset_index()



ax = df.plot(x='Date' , y='Close');
ax.set_xlabel("Date")
ax.set_ylabel("Close Price (USD)")



"""### Data Normalization"""

scaler = MinMaxScaler()

close_price = df.Close.values.reshape(-1, 1) # -1 in reshape function is used when you dont know or want to explicitly tell the dimension of that axis

scaled_close = scaler.fit_transform(close_price) # This method performs fit and transform on the input data at a single time and converts the data points



"""### Define Time2Vec + LSTM model"""

# The special syntax **kwargs in function definitions in python is used to pass a keyworded, variable-length argument list. 
# We use the name kwargs with the double star. The reason is because the double star allows us to pass through keyword arguments (and any number of them)



from tensorflow.keras import layers, Model, backend as K
from tensorflow.keras import backend as K # Keras is a model-level library, providing high-level building blocks for developing deep learning models. It does not handle itself low-level operations such as tensor products, convolutions and so on. Instead, it relies on a specialized, well-optimized tensor manipulation library to do so, serving as the “backend engine” of Keras.
from tensorflow.keras.layers import Layer

class T2V(Layer):
    
    def __init__(self, output_dim=None, **kwargs):
        self.output_dim = output_dim
        super(T2V, self).__init__(**kwargs)
        
    def build(self, input_shape):

        self.W = self.add_weight(name='W',
                                shape=(input_shape[-1], self.output_dim),
                                initializer='uniform',
                                trainable=True)

        self.P = self.add_weight(name='P',
                                shape=(input_shape[1], self.output_dim),
                                initializer='uniform',
                                trainable=True)

        self.w = self.add_weight(name='w',
                                shape=(input_shape[1], 1),
                                initializer='uniform',
                                trainable=True)

        self.p = self.add_weight(name='p',
                                shape=(input_shape[1], 1),
                                initializer='uniform',
                                trainable=True)

        super(T2V, self).build(input_shape)
        
    def call(self, x):
        
        original = self.w * x + self.p #if i = 0
        sin_trans = K.sin(K.dot(x, self.W) + self.P) # Frequecy and phase shift of sine function, learnable parameters. if 1 <= i <= k
        
        return K.concatenate([sin_trans, original], -1)

# To create X and Y for you
def gen_sequence(id_df, seq_length, seq_cols):
    
    data_matrix = id_df[seq_cols].values
    num_elements = data_matrix.shape[0]

    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):
        yield data_matrix[start:stop, :]

def gen_labels(id_df, seq_length, label):
    
    data_matrix = id_df[label].values
    num_elements = data_matrix.shape[0]
    
    return data_matrix[seq_length:num_elements, :]

def T2V_NN(param, dim):
    
    inp = layers.Input(shape=(dim,1))
    x = T2V(param['t2v_dim'])(inp)
    x = LSTM(param['unit'], activation=param['act'])(x)
    x = Dense(1)(x)
    
    m = Model(inp, x)
    m.compile(loss='mse', optimizer='adam')
    
    return m



"""### Prepare data to feed models"""

SEQ_LEN = 20 # pattern X is the size of Seq_len (e.g. use the first 20 days to predict 21st day)
X, Y = [], []
for sequence in gen_sequence(df, SEQ_LEN, ['Close']):
    X.append(sequence)
    
for sequence in gen_labels(df, SEQ_LEN, ['Close']):
    Y.append(sequence)
    
X = np.asarray(X)
Y = np.asarray(Y)



"""### Train Test Split"""

train_dim = int(0.7*len(df))
X_train, X_test = X[:train_dim], X[train_dim:]
y_train, y_test = Y[:train_dim], Y[train_dim:]

print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)



"""### Define parameter grid for hyperparameter optimization"""

param_grid = {
    'unit': [64,32],
    't2v_dim': [128,64,16],
    'lr': [1e-2,1e-3], 
    'act': ['elu','relu'], 
    'epochs': 20,
    'batch_size': [128,512,1024]
}

hypermodel = lambda x: T2V_NN(param=x, dim=SEQ_LEN)

kgs_t2v = KerasGridSearch(hypermodel, param_grid, monitor='val_loss', greater_is_better=False, tuner_verbose=1)
kgs_t2v.search(X_train, y_train, validation_split=0.2, shuffle=False)



"""### Application of the parameters coming from the Keras Grid Search with the best score"""

# Search({'unit': 32, 't2v_dim': 64, 'lr': 0.001, 'act': 'elu', 'epochs': 20, 'batch_size': 1024})

base_param = {
    'unit': 32,
    't2v_dim': 64,
    'lr': 1e-2, 
    'act': 'elu', 
    'epochs': 20,
    'batch_size': 1024
}

model = T2V_NN(param=base_param, dim=SEQ_LEN)

model.summary()

history = model.fit(X_train, y_train, epochs=20, validation_split=0.2, shuffle=False)



model.evaluate(X_test, y_test)



"""### Graph plot to see the loss variables vs epoch"""

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()



"""### Graph plot to see the actual vs predicted price"""

y_hat = model.predict(X_test)

# scale in a way that is easier to visualize in the graph (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)
# inverse_transform: Scale back the data to the original representation
y_test_inverse = scaler.inverse_transform(y_test)
y_hat_inverse = scaler.inverse_transform(y_hat)
 
plt.plot(y_test_inverse, label="Actual Price", color='green')
plt.plot(y_hat_inverse, label="Predicted Price", color='red')
 
plt.title('Ethereum price prediction')
plt.xlabel('Time [days]')
plt.ylabel('Price')
plt.legend(loc='best')
 
plt.show();

